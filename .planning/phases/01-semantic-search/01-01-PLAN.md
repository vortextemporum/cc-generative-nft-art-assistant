---
phase: 01-semantic-search
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - services/embeddings/model.js
  - services/embeddings/chunker.js
  - services/embeddings/storage.js
  - services/embeddings/generate.js
  - processed/embeddings/metadata.json
  - processed/embeddings/vectors.json
  - package.json
autonomous: true

must_haves:
  truths:
    - "Embedding model loads successfully and generates 384-dim vectors"
    - "All 28k projects are chunked into semantic units"
    - "Embeddings persist to disk and survive restarts"
    - "Progress is visible during generation"
  artifacts:
    - path: "services/embeddings/model.js"
      provides: "Model singleton with lazy loading"
      exports: ["EmbeddingModel", "getInstance", "embed", "embedBatch"]
    - path: "services/embeddings/chunker.js"
      provides: "Smart text chunking for projects"
      exports: ["chunkProject", "chunkProjects"]
    - path: "services/embeddings/storage.js"
      provides: "Vector persistence layer"
      exports: ["saveIndex", "loadIndex", "indexExists"]
    - path: "services/embeddings/generate.js"
      provides: "CLI for embedding generation"
      min_lines: 80
    - path: "processed/embeddings/vectors.json"
      provides: "Persisted embeddings for all projects"
    - path: "processed/embeddings/metadata.json"
      provides: "Index metadata (model, version, counts)"
  key_links:
    - from: "services/embeddings/generate.js"
      to: "services/embeddings/model.js"
      via: "import EmbeddingModel"
      pattern: "import.*EmbeddingModel.*from.*model"
    - from: "services/embeddings/generate.js"
      to: "processed/rag-documents.json"
      via: "reads project data"
      pattern: "rag-documents\\.json"
    - from: "services/embeddings/model.js"
      to: "@huggingface/transformers"
      via: "pipeline import"
      pattern: "import.*from.*@huggingface/transformers"
---

<objective>
Build the embedding generation infrastructure for semantic search.

Purpose: Transform the existing 28k project knowledge base into searchable embeddings using the bge-small-en-v1.5 model. This enables semantic similarity search that understands creative intent, not just keywords.

Output: Persisted embeddings (~80-120MB) ready for search, plus reusable embedding service modules.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-semantic-search/01-CONTEXT.md
@.planning/phases/01-semantic-search/01-RESEARCH.md

# Existing code to migrate/reference
@services/embeddings/embed.js
@processed/rag-documents.json (28,338 docs - source data)
@package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Upgrade transformers library and create model singleton</name>
  <files>
    package.json
    services/embeddings/model.js
  </files>
  <action>
    1. Update package.json: Replace `@xenova/transformers` with `@huggingface/transformers` (^3.x)

    2. Create services/embeddings/model.js with:
       - ESM syntax (import/export)
       - EmbeddingModel class with static singleton pattern
       - MODEL_ID = 'Xenova/bge-small-en-v1.5' (384 dims, MIT license)
       - getInstance(progressCallback) - lazy loads model, caches in .cache/models
       - embed(text) - returns Float32Array as regular array
       - embedBatch(texts, batchSize=50) - processes in batches with progress
       - Configure env.cacheDir = './.cache/models'
       - Use { pooling: 'mean', normalize: true } for all embeddings
       - Add progress_callback for download status (first run downloads ~33MB)

    Reference 01-RESEARCH.md Pattern 1 for implementation details.

    IMPORTANT: Use ESM syntax - package.json already has "type": "module"
  </action>
  <verify>
    Run: `npm install && node -e "import('./services/embeddings/model.js').then(m => m.EmbeddingModel.getInstance().then(() => console.log('Model loaded')))"`
    Should output download progress (first run) then "Model loaded"
  </verify>
  <done>
    - @huggingface/transformers installed
    - Model singleton loads bge-small-en-v1.5
    - embed() returns 384-dimensional normalized vector
    - embedBatch() processes arrays with progress output
  </done>
</task>

<task type="auto">
  <name>Task 2: Create chunker and storage modules</name>
  <files>
    services/embeddings/chunker.js
    services/embeddings/storage.js
  </files>
  <action>
    1. Create services/embeddings/chunker.js:
       - chunkProject(project) - returns array of chunk objects:
         - Chunk 1 (metadata): "{name} by {artist}. {description (max 400 chars)}"
         - Chunk 2 (tags): "Tags: {aesthetics}, {patterns}" (if exist)
         - Each chunk has: { type, content, projectId, source, artist, scriptType }
       - chunkProjects(projects) - maps over array, flattens results
       - Target: 1-2 chunks per project (metadata always, tags if substantial)
       - Keep chunks under 400 words (~512 tokens) for model context window

    2. Create services/embeddings/storage.js:
       - INDEX_DIR = './processed/embeddings'
       - VECTORS_PATH = './processed/embeddings/vectors.json'
       - METADATA_PATH = './processed/embeddings/metadata.json'
       - ensureDir() - creates embeddings directory if needed
       - saveIndex({ vectors, metadata }) - writes both files
       - loadIndex() - returns { vectors, metadata } or null
       - indexExists() - boolean check
       - Metadata structure: { model, dimensions: 384, created, projectCount, chunkCount }
       - Vectors structure: array of { id, projectId, type, embedding, source, artist, scriptType }

    Reference 01-RESEARCH.md Pattern 2 and Pattern 4 for structure.
  </action>
  <verify>
    Run: `node -e "import('./services/embeddings/chunker.js').then(c => console.log(c.chunkProject({ id: 'test', name: 'Test', artist_name: 'Artist', description: 'A test project' })))"`
    Should output array with metadata chunk

    Run: `node -e "import('./services/embeddings/storage.js').then(s => { s.ensureDir(); console.log('Storage ready') })"`
    Should create processed/embeddings/ directory
  </verify>
  <done>
    - chunker.js exports chunkProject, chunkProjects
    - Chunks include projectId, type, source metadata
    - storage.js handles read/write of vectors and metadata
    - Directory structure created at processed/embeddings/
  </done>
</task>

<task type="auto">
  <name>Task 3: Create embedding generator and run initial generation</name>
  <files>
    services/embeddings/generate.js
    processed/embeddings/vectors.json
    processed/embeddings/metadata.json
  </files>
  <action>
    1. Create services/embeddings/generate.js as CLI script:
       - Import model, chunker, storage modules
       - Load RAG documents from processed/rag-documents.json
       - Chunk all projects using chunker.chunkProjects()
       - Embed all chunks in batches of 50 (to avoid memory issues)
       - Show progress: "Embedding chunk X/Y (Z%)"
       - Write to storage periodically (every 1000 chunks) as checkpoint
       - Final save with complete metadata
       - Add --force flag to regenerate even if index exists
       - Add --limit N flag for testing with subset
       - Estimate: ~30-60 minutes for full 28k projects

    2. Run generator with small test first:
       `node services/embeddings/generate.js --limit 100`

    3. If test passes, run full generation:
       `node services/embeddings/generate.js`

    Memory management:
    - Process in batches, don't hold all tensors
    - Write checkpoints to disk during generation
    - Clear references after each batch

    Expected output size: ~80-120MB for vectors.json
  </action>
  <verify>
    Test run: `node services/embeddings/generate.js --limit 100`
    Should complete without errors, create vectors.json with ~100-200 vectors

    Check structure: `node -e "import('./services/embeddings/storage.js').then(s => { const i = s.loadIndex(); console.log('Vectors:', i?.vectors?.length, 'Meta:', i?.metadata) })"`
    Should show vector count and metadata object

    Full run: `node services/embeddings/generate.js`
    Wait for completion (~30-60 min), should show final count ~28-42k chunks
  </verify>
  <done>
    - generate.js CLI creates embeddings from RAG documents
    - --limit flag works for testing
    - vectors.json contains embeddings for all 28k projects
    - metadata.json contains model info and counts
    - File size reasonable (80-120MB)
  </done>
</task>

</tasks>

<verification>
1. `npm ls @huggingface/transformers` shows package installed
2. `ls -la processed/embeddings/` shows vectors.json and metadata.json
3. `node -e "import('./services/embeddings/storage.js').then(s => console.log(s.loadIndex()?.metadata))"` shows correct metadata
4. vectors.json size is between 80-150MB (28k+ vectors * 384 dims * 4 bytes + overhead)
</verification>

<success_criteria>
1. Model loads and generates 384-dimensional normalized embeddings
2. All 28,338 projects have been chunked and embedded
3. Embeddings persist in processed/embeddings/ directory
4. Can load embeddings back from disk without regeneration
5. Progress visible during generation (not silent for 30+ minutes)
</success_criteria>

<output>
After completion, create `.planning/phases/01-semantic-search/01-01-SUMMARY.md`
</output>
